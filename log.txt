Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting tensorboardX
  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)
     |████████████████████████████████| 125 kB 4.1 MB/s
Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)
Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)
Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)
Installing collected packages: tensorboardX
Successfully installed tensorboardX-2.5.1
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting gpustat
  Downloading gpustat-0.6.0.tar.gz (78 kB)
     |████████████████████████████████| 78 kB 3.2 MB/s
Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from gpustat) (1.15.0)
Collecting nvidia-ml-py3>=7.352.0
  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)
Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat) (5.4.8)
Collecting blessings>=1.6
  Downloading blessings-1.7-py3-none-any.whl (18 kB)
Building wheels for collected packages: gpustat, nvidia-ml-py3
  Building wheel for gpustat (setup.py) ... done
  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=f1b8eaab82f7795099b8d197898a16484201c1d276217084f66a6b9c005b3331
  Stored in directory: /root/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8
  Building wheel for nvidia-ml-py3 (setup.py) ... done
  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19190 sha256=eca95233ad1a9d4dafdf7aeb43f6bef322fa0ac4fb1e79b9daf7fa5a145643eb
  Stored in directory: /root/.cache/pip/wheels/df/99/da/c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83
Successfully built gpustat nvidia-ml-py3
Installing collected packages: nvidia-ml-py3, blessings, gpustat
Successfully installed blessings-1.7 gpustat-0.6.0 nvidia-ml-py3-7.352.0
2022-06-25 14:38:00 running arguments: Namespace(batch_size=128, bit_width_list='4,4,4,4,4', dataset='cifar10', epochs=200, lr=0.001, lr_decay='100,150,180', model='resnet20q', optimizer='adam', pretrain=None, print_freq=20, results_dir='/content/drive/MyDrive/Any-Precision-DNNs/results/exp_cifar10_resnet20q_124832_recursive_20220625_143746', resume=None, start_epoch=0, train_split='train', weight_decay=0.0, workers=0)
Files already downloaded and verified
Files already downloaded and verified
2022-06-25 14:38:24 number of parameters: 6758810
2022-06-25 14:38:24 input (fake quantized):
2022-06-25 14:38:24 tensor([[[[-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-1.9201, -1.9201, -1.6458,  ...,  0.2743,  0.5486,  0.2743],
          [-1.9201, -1.9201, -1.3715,  ..., -0.2743,  0.0000,  0.0000],
          [-1.9201, -1.9201, -1.0972,  ..., -0.2743, -0.2743, -0.2743]],

         [[-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-1.9201, -1.9201, -1.3715,  ...,  0.2743,  0.5486,  0.5486],
          [-1.9201, -1.9201, -1.3715,  ...,  0.0000,  0.2743,  0.0000],
          [-1.9201, -1.9201, -1.0972,  ..., -0.2743, -0.2743, -0.2743]],

         [[-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          ...,
          [-1.6458, -1.6458, -1.0972,  ...,  0.0000,  0.2743,  0.2743],
          [-1.6458, -1.6458, -1.0972,  ..., -0.2743,  0.0000,  0.0000],
          [-1.6458, -1.6458, -0.8229,  ..., -0.5486, -0.5486, -0.2743]]],


        [[[-1.9201, -1.9201,  0.0000,  ...,  0.2743,  0.5486,  0.2743],
          [-1.9201, -1.9201,  0.0000,  ...,  0.2743,  0.2743,  0.2743],
          [-1.9201, -1.9201, -0.2743,  ...,  0.0000,  0.2743,  0.2743],
          ...,
          [-1.9201, -1.9201,  0.2743,  ...,  0.2743,  0.0000,  0.0000],
          [-1.9201, -1.9201,  0.2743,  ...,  0.2743,  0.0000,  0.0000],
          [-1.9201, -1.9201,  0.0000,  ...,  0.2743,  0.2743,  0.2743]],

         [[-1.9201, -1.9201,  0.2743,  ...,  0.2743,  0.2743,  0.2743],
          [-1.9201, -1.9201,  0.2743,  ...,  0.2743,  0.2743,  0.2743],
          [-1.9201, -1.9201,  0.0000,  ...,  0.0000,  0.0000,  0.2743],
          ...,
          [-1.9201, -1.9201,  0.2743,  ...,  0.2743,  0.2743,  0.2743],
          [-1.9201, -1.9201,  0.5486,  ...,  0.5486,  0.2743,  0.2743],
          [-1.9201, -1.9201,  0.0000,  ...,  0.5486,  0.2743,  0.2743]],

         [[-1.6458, -1.6458, -0.8229,  ..., -0.5486, -0.2743, -0.5486],
          [-1.6458, -1.6458, -0.5486,  ..., -0.8229, -0.5486, -0.5486],
          [-1.6458, -1.6458, -0.8229,  ..., -0.8229, -0.5486, -0.5486],
          ...,
          [-1.6458, -1.6458, -0.5486,  ..., -0.5486, -0.8229, -0.5486],
          [-1.6458, -1.6458, -0.5486,  ..., -0.2743, -0.5486, -0.5486],
          [-1.6458, -1.6458, -0.8229,  ..., -0.2743, -0.5486, -0.5486]]],


        [[[ 0.5486,  0.5486,  0.2743,  ..., -1.9201, -1.9201, -1.9201],
          [ 0.5486,  0.5486,  0.5486,  ..., -1.9201, -1.9201, -1.9201],
          [ 0.8229,  0.8229,  0.8229,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-0.8229, -0.2743, -0.8229,  ..., -1.9201, -1.9201, -1.9201],
          [-0.5486,  0.0000, -0.8229,  ..., -1.9201, -1.9201, -1.9201],
          [-0.5486, -0.2743, -0.5486,  ..., -1.9201, -1.9201, -1.9201]],

         [[ 0.0000,  0.0000, -0.2743,  ..., -1.9201, -1.9201, -1.9201],
          [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
          [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-1.0972, -0.8229, -1.0972,  ..., -1.9201, -1.9201, -1.9201],
          [-1.0972, -0.5486, -1.0972,  ..., -1.9201, -1.9201, -1.9201],
          [-0.8229, -0.8229, -1.0972,  ..., -1.9201, -1.9201, -1.9201]],

         [[-0.5486, -0.5486, -0.8229,  ..., -1.6458, -1.6458, -1.6458],
          [-0.5486, -0.5486, -0.8229,  ..., -1.6458, -1.6458, -1.6458],
          [-0.5486, -0.5486, -0.5486,  ..., -1.6458, -1.6458, -1.6458],
          ...,
          [-1.3715, -1.0972, -1.3715,  ..., -1.6458, -1.6458, -1.6458],
          [-1.0972, -0.8229, -1.3715,  ..., -1.6458, -1.6458, -1.6458],
          [-0.8229, -1.0972, -1.0972,  ..., -1.6458, -1.6458, -1.6458]]],


        ...,


        [[[-0.8229, -0.5486, -0.5486,  ..., -1.9201, -1.9201, -1.9201],
          [-0.5486, -0.5486, -0.5486,  ..., -1.9201, -1.9201, -1.9201],
          [-0.5486, -0.5486, -0.5486,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-0.2743, -0.2743,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201]],

         [[-1.0972, -1.0972, -0.8229,  ..., -1.9201, -1.9201, -1.9201],
          [-0.8229, -1.0972, -0.8229,  ..., -1.9201, -1.9201, -1.9201],
          [-0.8229, -0.8229, -0.8229,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-0.5486, -0.5486, -0.5486,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201]],

         [[-0.8229, -0.8229, -0.8229,  ..., -1.6458, -1.6458, -1.6458],
          [-0.8229, -0.8229, -0.8229,  ..., -1.6458, -1.6458, -1.6458],
          [-0.8229, -0.8229, -0.8229,  ..., -1.6458, -1.6458, -1.6458],
          ...,
          [-0.5486, -0.8229, -0.5486,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458]]],


        [[[-1.9201, -1.9201,  0.8229,  ...,  1.0972,  0.8229,  1.0972],
          [-1.9201, -1.9201,  0.8229,  ...,  1.0972,  1.0972,  1.0972],
          [-1.9201, -1.9201,  0.8229,  ...,  1.0972,  1.0972,  1.0972],
          ...,
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201]],

         [[-1.9201, -1.9201,  1.0972,  ...,  1.0972,  1.0972,  1.0972],
          [-1.9201, -1.9201,  1.0972,  ...,  1.0972,  1.0972,  1.0972],
          [-1.9201, -1.9201,  1.0972,  ...,  1.0972,  1.0972,  1.0972],
          ...,
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201]],

         [[-1.6458, -1.6458,  1.0972,  ...,  1.0972,  1.0972,  1.0972],
          [-1.6458, -1.6458,  1.0972,  ...,  1.0972,  1.0972,  1.0972],
          [-1.6458, -1.6458,  1.0972,  ...,  1.0972,  1.0972,  1.0972],
          ...,
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458]]],


        [[[-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [ 0.0000,  0.0000,  0.2743,  ...,  0.5486,  0.5486,  0.5486],
          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.2743, -0.5486],
          [ 0.2743,  0.2743,  0.0000,  ..., -1.0972, -1.0972, -1.0972]],

         [[-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [ 0.2743,  0.2743,  0.2743,  ...,  0.8229,  0.5486,  0.5486],
          [ 0.2743,  0.2743,  0.2743,  ...,  0.0000, -0.2743, -0.5486],
          [ 0.5486,  0.5486,  0.5486,  ..., -0.8229, -1.0972, -0.8229]],

         [[-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          ...,
          [-0.2743, -0.2743, -0.2743,  ...,  0.0000,  0.0000,  0.0000],
          [-0.2743, -0.5486, -0.5486,  ..., -0.2743, -0.5486, -0.8229],
          [-0.2743, -0.2743, -0.2743,  ..., -0.8229, -1.0972, -0.8229]]]],
       device='cuda:0')
2022-06-25 14:38:24 weight (fake quantized):
2022-06-25 14:38:24 tensor([[[[-0.1026, -0.1026, -0.0769],
          [ 0.0256,  0.0513,  0.1795],
          [ 0.0513, -0.1282, -0.1539]],

         [[-0.1282,  0.1282,  0.0769],
          [-0.0769,  0.1026, -0.1026],
          [-0.1282, -0.1026, -0.0256]],

         [[-0.0256, -0.1026, -0.1026],
          [-0.1026,  0.0000,  0.0513],
          [ 0.0513,  0.0769, -0.0513]]],


        [[[ 0.0513,  0.1795,  0.0513],
          [ 0.0000, -0.0256, -0.1539],
          [-0.1539, -0.0769,  0.1795]],

         [[-0.0256, -0.1026, -0.1539],
          [ 0.1539, -0.1539, -0.1539],
          [-0.1795, -0.1282, -0.0513]],

         [[ 0.1539, -0.1539,  0.1026],
          [ 0.0256, -0.0513, -0.1539],
          [-0.0256,  0.0256,  0.0256]]],


        [[[ 0.1026,  0.0513, -0.1795],
          [-0.0513,  0.0769, -0.0769],
          [-0.0256,  0.0256, -0.1282]],

         [[-0.1795,  0.0513,  0.1795],
          [-0.1795,  0.1026,  0.1795],
          [-0.0769, -0.1795, -0.1282]],

         [[ 0.1282, -0.1026, -0.0513],
          [-0.0256,  0.0513, -0.0513],
          [ 0.1539,  0.1026,  0.0513]]],


        ...,


        [[[ 0.1795, -0.1539,  0.0513],
          [ 0.0513, -0.1795, -0.0513],
          [ 0.0256, -0.1795, -0.0513]],

         [[-0.1539,  0.1282,  0.0256],
          [ 0.1795,  0.1795,  0.0000],
          [-0.1282, -0.0256,  0.0000]],

         [[ 0.1282,  0.1282,  0.0769],
          [-0.1795,  0.1026,  0.0000],
          [-0.0513, -0.0513, -0.0769]]],


        [[[-0.1795, -0.0256, -0.0513],
          [ 0.1282, -0.0513, -0.0256],
          [ 0.1795,  0.1539,  0.0513]],

         [[-0.1026,  0.1282,  0.1795],
          [ 0.1026, -0.0769, -0.0769],
          [ 0.1026, -0.1026,  0.1539]],

         [[ 0.1539,  0.1282,  0.0000],
          [-0.1795, -0.1795,  0.1539],
          [ 0.1795,  0.1026,  0.0000]]],


        [[[-0.0513,  0.0000, -0.0256],
          [ 0.1282, -0.1026,  0.1795],
          [ 0.0769, -0.1539, -0.1539]],

         [[ 0.1795, -0.0769, -0.0256],
          [-0.1539,  0.1539, -0.1282],
          [-0.0769,  0.1282,  0.0000]],

         [[ 0.0256,  0.1282,  0.0513],
          [ 0.1282, -0.1026, -0.0769],
          [-0.0513, -0.0769, -0.0513]]]], device='cuda:0',
       grad_fn=<FakeQuantOpBackward>)
2022-06-25 14:38:24 input (fake quantized) unfold:
2022-06-25 14:38:24 tensor([[[ 0.0000,  0.0000,  0.0000,  ..., -0.5486, -0.2743,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ..., -0.2743,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         ...,
         [ 0.0000, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.2743,  0.2743,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.2743,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         ...,
         [ 0.0000, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458, -0.5486,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -0.5486, -0.8229,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201,  0.0000],
         ...,
         [ 0.0000, -0.5486, -0.5486,  ...,  0.0000,  0.0000,  0.0000],
         [-0.5486, -0.5486, -0.8229,  ...,  0.0000,  0.0000,  0.0000],
         [-0.5486, -0.8229, -0.5486,  ...,  0.0000,  0.0000,  0.0000]],

        ...,

        [[ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201,  0.0000],
         ...,
         [ 0.0000, -0.8229, -0.8229,  ...,  0.0000,  0.0000,  0.0000],
         [-0.8229, -0.8229, -0.8229,  ...,  0.0000,  0.0000,  0.0000],
         [-0.8229, -0.8229, -0.8229,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201,  0.0000],
         ...,
         [ 0.0000, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458,  1.0972,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458,  1.0972,  1.0972,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.2743],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.2743, -0.5486],
         [ 0.0000,  0.0000,  0.0000,  ..., -0.2743, -0.5486,  0.0000],
         ...,
         [ 0.0000, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:0')
2022-06-25 14:38:24 weight (fake quantized) unfold:
2022-06-25 14:38:24 tensor([[-0.1026,  0.0513,  0.1026,  ...,  0.1795, -0.1795, -0.0513],
        [-0.1026,  0.1795,  0.0513,  ..., -0.1539, -0.0256,  0.0000],
        [-0.0769,  0.0513, -0.1795,  ...,  0.0513, -0.0513, -0.0256],
        ...,
        [ 0.0513, -0.0256,  0.1539,  ..., -0.0513,  0.1795, -0.0513],
        [ 0.0769,  0.0256,  0.1026,  ..., -0.0513,  0.1026, -0.0769],
        [-0.0513,  0.0256,  0.0513,  ..., -0.0769,  0.0000, -0.0513]],
       device='cuda:0', grad_fn=<TBackward0>)
2022-06-25 14:38:24 input (fake quantized) unfold.shape:
2022-06-25 14:38:24 torch.Size([128, 27, 1024])
2022-06-25 14:38:24 weight (fake quantized) unfold.shape:
2022-06-25 14:38:24 torch.Size([27, 80])
2022-06-25 14:38:24 input qtensor:
2022-06-25 14:38:24 QTensor(tensor=tensor([[[[ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          ...,
          [ 0,  0,  1,  ...,  8,  9,  8],
          [ 0,  0,  2,  ...,  6,  7,  7],
          [ 0,  0,  3,  ...,  6,  6,  6]],

         [[ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          ...,
          [ 0,  0,  2,  ...,  8,  9,  9],
          [ 0,  0,  2,  ...,  7,  8,  7],
          [ 0,  0,  3,  ...,  6,  6,  6]],

         [[ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          ...,
          [ 1,  1,  3,  ...,  7,  8,  8],
          [ 1,  1,  3,  ...,  6,  7,  7],
          [ 1,  1,  4,  ...,  5,  5,  6]]],


        [[[ 0,  0,  7,  ...,  8,  9,  8],
          [ 0,  0,  7,  ...,  8,  8,  8],
          [ 0,  0,  6,  ...,  7,  8,  8],
          ...,
          [ 0,  0,  8,  ...,  8,  7,  7],
          [ 0,  0,  8,  ...,  8,  7,  7],
          [ 0,  0,  7,  ...,  8,  8,  8]],

         [[ 0,  0,  8,  ...,  8,  8,  8],
          [ 0,  0,  8,  ...,  8,  8,  8],
          [ 0,  0,  7,  ...,  7,  7,  8],
          ...,
          [ 0,  0,  8,  ...,  8,  8,  8],
          [ 0,  0,  9,  ...,  9,  8,  8],
          [ 0,  0,  7,  ...,  9,  8,  8]],

         [[ 1,  1,  4,  ...,  5,  6,  5],
          [ 1,  1,  5,  ...,  4,  5,  5],
          [ 1,  1,  4,  ...,  4,  5,  5],
          ...,
          [ 1,  1,  5,  ...,  5,  4,  5],
          [ 1,  1,  5,  ...,  6,  5,  5],
          [ 1,  1,  4,  ...,  6,  5,  5]]],


        [[[ 9,  9,  8,  ...,  0,  0,  0],
          [ 9,  9,  9,  ...,  0,  0,  0],
          [10, 10, 10,  ...,  0,  0,  0],
          ...,
          [ 4,  6,  4,  ...,  0,  0,  0],
          [ 5,  7,  4,  ...,  0,  0,  0],
          [ 5,  6,  5,  ...,  0,  0,  0]],

         [[ 7,  7,  6,  ...,  0,  0,  0],
          [ 7,  7,  7,  ...,  0,  0,  0],
          [ 7,  7,  7,  ...,  0,  0,  0],
          ...,
          [ 3,  4,  3,  ...,  0,  0,  0],
          [ 3,  5,  3,  ...,  0,  0,  0],
          [ 4,  4,  3,  ...,  0,  0,  0]],

         [[ 5,  5,  4,  ...,  1,  1,  1],
          [ 5,  5,  4,  ...,  1,  1,  1],
          [ 5,  5,  5,  ...,  1,  1,  1],
          ...,
          [ 2,  3,  2,  ...,  1,  1,  1],
          [ 3,  4,  2,  ...,  1,  1,  1],
          [ 4,  3,  3,  ...,  1,  1,  1]]],


        ...,


        [[[ 4,  5,  5,  ...,  0,  0,  0],
          [ 5,  5,  5,  ...,  0,  0,  0],
          [ 5,  5,  5,  ...,  0,  0,  0],
          ...,
          [ 6,  6,  7,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0]],

         [[ 3,  3,  4,  ...,  0,  0,  0],
          [ 4,  3,  4,  ...,  0,  0,  0],
          [ 4,  4,  4,  ...,  0,  0,  0],
          ...,
          [ 5,  5,  5,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0]],

         [[ 4,  4,  4,  ...,  1,  1,  1],
          [ 4,  4,  4,  ...,  1,  1,  1],
          [ 4,  4,  4,  ...,  1,  1,  1],
          ...,
          [ 5,  4,  5,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1]]],


        [[[ 0,  0, 10,  ..., 11, 10, 11],
          [ 0,  0, 10,  ..., 11, 11, 11],
          [ 0,  0, 10,  ..., 11, 11, 11],
          ...,
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0]],

         [[ 0,  0, 11,  ..., 11, 11, 11],
          [ 0,  0, 11,  ..., 11, 11, 11],
          [ 0,  0, 11,  ..., 11, 11, 11],
          ...,
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0]],

         [[ 1,  1, 11,  ..., 11, 11, 11],
          [ 1,  1, 11,  ..., 11, 11, 11],
          [ 1,  1, 11,  ..., 11, 11, 11],
          ...,
          [ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1]]],


        [[[ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          ...,
          [ 7,  7,  8,  ...,  9,  9,  9],
          [ 7,  7,  7,  ...,  7,  6,  5],
          [ 8,  8,  7,  ...,  3,  3,  3]],

         [[ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          ...,
          [ 8,  8,  8,  ..., 10,  9,  9],
          [ 8,  8,  8,  ...,  7,  6,  5],
          [ 9,  9,  9,  ...,  4,  3,  4]],

         [[ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          ...,
          [ 6,  6,  6,  ...,  7,  7,  7],
          [ 6,  5,  5,  ...,  6,  5,  4],
          [ 6,  6,  6,  ...,  4,  3,  4]]]], device='cuda:0',
       dtype=torch.uint8), scale=tensor(0.2743, device='cuda:0'), zero_point=7)
2022-06-25 14:38:24 weight qtensor:
2022-06-25 14:38:24 QTensor(tensor=tensor([[[[ 3,  3,  4],
          [ 8,  9, 14],
          [ 9,  2,  1]],

         [[ 2, 12, 10],
          [ 4, 11,  3],
          [ 2,  3,  6]],

         [[ 6,  3,  3],
          [ 3,  7,  9],
          [ 9, 10,  5]]],


        [[[ 9, 14,  9],
          [ 7,  6,  1],
          [ 1,  4, 14]],

         [[ 6,  3,  1],
          [13,  1,  1],
          [ 0,  2,  5]],

         [[13,  1, 11],
          [ 8,  5,  1],
          [ 6,  8,  8]]],


        [[[11,  9,  0],
          [ 5, 10,  4],
          [ 6,  8,  2]],

         [[ 0,  9, 14],
          [ 0, 11, 14],
          [ 4,  0,  2]],

         [[12,  3,  5],
          [ 6,  9,  5],
          [13, 11,  9]]],


        ...,


        [[[14,  1,  9],
          [ 9,  0,  5],
          [ 8,  0,  5]],

         [[ 1, 12,  8],
          [14, 14,  7],
          [ 2,  6,  7]],

         [[12, 12, 10],
          [ 0, 11,  7],
          [ 5,  5,  4]]],


        [[[ 0,  6,  5],
          [12,  5,  6],
          [14, 13,  9]],

         [[ 3, 12, 14],
          [11,  4,  4],
          [11,  3, 13]],

         [[13, 12,  7],
          [ 0,  0, 13],
          [14, 11,  7]]],


        [[[ 5,  7,  6],
          [12,  3, 14],
          [10,  1,  1]],

         [[14,  4,  6],
          [ 1, 13,  2],
          [ 4, 12,  7]],

         [[ 8, 12,  9],
          [12,  3,  4],
          [ 5,  4,  5]]]], device='cuda:0', dtype=torch.uint8), scale=tensor(0.0256, device='cuda:0'), zero_point=7)
Traceback (most recent call last):
  File "/content/drive/MyDrive/Any-Precision-DNNs/train.py", line 222, in <module>
    main()
  File "/content/drive/MyDrive/Any-Precision-DNNs/train.py", line 110, in main
    optimizer, sum_writer)
  File "/content/drive/MyDrive/Any-Precision-DNNs/train.py", line 185, in forward
    output = model(input)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/content/drive/MyDrive/Any-Precision-DNNs/models/resnet_quan.py", line 110, in forward
    out = self.conv0(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/content/drive/MyDrive/Any-Precision-DNNs/models/quan_ops.py", line 307, in forward
    return myconv2d_lut(input_qtensor, weight_qtensor, input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
  File "/content/drive/MyDrive/Any-Precision-DNNs/models/quan_ops.py", line 270, in myconv2d_lut
    inp_qtensor_unf = unfold_qtensor(input_qtensor)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/fold.py", line 295, in forward
    self.padding, self.stride)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py", line 4674, in unfold
    if input.dim() == 4:
AttributeError: 'QTensor' object has no attribute 'dim'
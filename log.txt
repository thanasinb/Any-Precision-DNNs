Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.5.1)
Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)
Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)
Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: gpustat in /usr/local/lib/python3.7/dist-packages (0.6.0)
Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat) (7.352.0)
Requirement already satisfied: blessings>=1.6 in /usr/local/lib/python3.7/dist-packages (from gpustat) (1.7)
Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat) (5.4.8)
Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from gpustat) (1.15.0)
2022-06-25 07:24:45 running arguments: Namespace(batch_size=128, bit_width_list='4,4,4,4,4', dataset='cifar10', epochs=200, lr=0.001, lr_decay='100,150,180', model='resnet20q', optimizer='adam', pretrain=None, print_freq=20, results_dir='/content/drive/MyDrive/Any-Precision-DNNs/results/exp_cifar10_resnet20q_124832_recursive_20220625_072443', resume=None, start_epoch=0, train_split='train', weight_decay=0.0, workers=0)
Files already downloaded and verified
Files already downloaded and verified
2022-06-25 07:24:52 number of parameters: 6758810
2022-06-25 07:24:52 input (fake quantized):
2022-06-25 07:24:52 tensor([[[[-1.9201,  1.6458,  1.6458,  ..., -0.8229, -0.8229, -1.0972],
          [-1.9201,  0.5486,  1.0972,  ..., -0.5486, -0.5486, -0.8229],
          [-1.9201,  0.0000,  0.0000,  ...,  0.0000, -0.5486, -0.8229],
          ...,
          [-1.9201,  0.8229,  0.2743,  ...,  0.5486,  1.3715,  1.3715],
          [-1.9201,  0.8229,  0.5486,  ...,  0.8229,  1.0972,  1.0972],
          [-1.9201,  1.3715,  1.3715,  ..., -0.2743,  0.5486,  0.5486]],

         [[-1.9201,  1.6458,  1.6458,  ..., -0.8229, -0.8229, -0.8229],
          [-1.9201,  0.5486,  1.0972,  ..., -0.5486, -0.5486, -0.5486],
          [-1.9201,  0.0000,  0.0000,  ...,  0.0000, -0.5486, -0.5486],
          ...,
          [-1.9201,  0.8229,  0.2743,  ...,  0.8229,  1.3715,  1.3715],
          [-1.9201,  0.8229,  0.5486,  ...,  0.8229,  1.0972,  1.0972],
          [-1.9201,  1.3715,  1.3715,  ..., -0.2743,  0.5486,  0.8229]],

         [[-1.6458,  1.6458,  1.6458,  ..., -0.5486, -0.5486, -0.8229],
          [-1.6458,  0.8229,  1.0972,  ..., -0.2743, -0.2743, -0.5486],
          [-1.6458,  0.0000,  0.2743,  ...,  0.2743, -0.5486, -0.5486],
          ...,
          [-1.6458,  0.8229,  0.2743,  ...,  0.8229,  1.3715,  1.3715],
          [-1.6458,  1.0972,  0.8229,  ...,  0.8229,  1.0972,  1.0972],
          [-1.6458,  1.3715,  1.3715,  ...,  0.0000,  0.5486,  0.8229]]],


        [[[ 0.0000,  0.2743,  0.5486,  ...,  0.5486,  0.2743,  0.2743],
          [ 0.5486,  0.8229,  0.5486,  ...,  0.5486,  0.5486,  0.2743],
          [ 1.3715,  1.0972,  0.0000,  ...,  0.5486,  0.5486,  0.5486],
          ...,
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201]],

         [[ 1.0972,  1.0972,  1.0972,  ...,  1.3715,  1.3715,  1.3715],
          [ 1.3715,  1.3715,  1.0972,  ...,  1.3715,  1.3715,  1.3715],
          [ 1.6458,  1.3715,  0.2743,  ...,  1.3715,  1.3715,  1.3715],
          ...,
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201]],

         [[ 1.9201,  1.9201,  1.9201,  ...,  2.1944,  2.1944,  2.1944],
          [ 1.9201,  1.9201,  1.3715,  ...,  2.1944,  1.9201,  2.1944],
          [ 2.1944,  1.6458,  0.2743,  ...,  2.1944,  2.1944,  2.1944],
          ...,
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458]]],


        [[[-1.9201,  0.5486,  0.5486,  ...,  0.5486,  0.5486,  0.5486],
          [-1.9201,  0.5486,  0.5486,  ...,  0.5486,  0.5486,  0.2743],
          [-1.9201,  0.5486,  0.5486,  ...,  0.5486,  0.5486,  0.5486],
          ...,
          [-1.9201,  1.3715,  1.3715,  ...,  0.2743,  0.2743,  0.2743],
          [-1.9201,  1.6458,  1.6458,  ...,  0.2743,  0.2743,  0.2743],
          [-1.9201,  1.3715,  1.3715,  ...,  0.2743,  0.2743,  0.2743]],

         [[-1.9201,  1.0972,  1.0972,  ...,  1.0972,  1.0972,  1.0972],
          [-1.9201,  1.3715,  1.3715,  ...,  1.3715,  1.3715,  1.3715],
          [-1.9201,  1.3715,  1.3715,  ...,  1.3715,  1.3715,  1.3715],
          ...,
          [-1.9201,  1.3715,  1.3715,  ...,  0.2743,  0.2743,  0.5486],
          [-1.9201,  1.6458,  1.6458,  ...,  0.5486,  0.2743,  0.2743],
          [-1.9201,  1.6458,  1.6458,  ...,  0.2743,  0.2743,  0.2743]],

         [[-1.6458,  1.6458,  1.6458,  ...,  1.6458,  1.6458,  1.6458],
          [-1.6458,  1.9201,  1.9201,  ...,  1.9201,  1.9201,  1.9201],
          [-1.6458,  1.9201,  1.9201,  ...,  1.9201,  1.9201,  1.9201],
          ...,
          [-1.6458,  1.3715,  1.3715,  ...,  0.5486,  0.5486,  0.5486],
          [-1.6458,  1.6458,  1.6458,  ...,  0.5486,  0.5486,  0.2743],
          [-1.6458,  1.6458,  1.6458,  ...,  0.5486,  0.2743,  0.2743]]],


        ...,


        [[[-0.2743, -0.2743, -0.2743,  ..., -1.9201, -1.9201, -1.9201],
          [-0.2743, -0.2743, -0.2743,  ..., -1.9201, -1.9201, -1.9201],
          [-0.2743, -0.2743, -0.2743,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-0.2743,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
          [-0.5486, -0.2743, -0.2743,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201]],

         [[-0.8229, -0.8229, -0.8229,  ..., -1.9201, -1.9201, -1.9201],
          [-0.8229, -0.8229, -0.8229,  ..., -1.9201, -1.9201, -1.9201],
          [-0.8229, -0.8229, -0.8229,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-0.5486, -0.2743, -0.2743,  ..., -1.9201, -1.9201, -1.9201],
          [-1.0972, -0.8229, -0.8229,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201]],

         [[-0.8229, -0.8229, -0.8229,  ..., -1.6458, -1.6458, -1.6458],
          [-0.8229, -0.8229, -0.8229,  ..., -1.6458, -1.6458, -1.6458],
          [-0.8229, -0.8229, -0.8229,  ..., -1.6458, -1.6458, -1.6458],
          ...,
          [-1.0972, -0.8229, -0.8229,  ..., -1.6458, -1.6458, -1.6458],
          [-1.3715, -1.3715, -1.3715,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458]]],


        [[[-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-1.9201,  0.0000,  0.0000,  ..., -0.8229,  0.2743,  0.0000],
          [-1.9201, -0.2743, -0.2743,  ..., -0.8229,  0.2743,  0.0000],
          [-1.9201,  0.0000,  0.0000,  ..., -0.5486, -0.2743,  0.0000]],

         [[-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-1.9201,  0.0000,  0.0000,  ..., -1.0972,  0.0000, -0.2743],
          [-1.9201, -0.2743, -0.2743,  ..., -0.8229,  0.0000, -0.2743],
          [-1.9201, -0.2743,  0.0000,  ..., -0.5486, -0.2743,  0.0000]],

         [[-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          ...,
          [-1.6458, -0.2743, -0.2743,  ..., -0.8229,  0.0000, -0.2743],
          [-1.6458, -0.2743, -0.2743,  ..., -0.5486,  0.0000,  0.0000],
          [-1.6458,  0.0000,  0.0000,  ..., -0.2743,  0.0000,  0.0000]]],


        [[[-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-0.5486, -0.8229, -0.8229,  ..., -0.8229, -1.9201, -1.9201],
          [-0.5486, -0.5486, -0.5486,  ..., -0.8229, -1.9201, -1.9201],
          [-0.5486, -0.5486, -0.2743,  ..., -0.5486, -1.9201, -1.9201]],

         [[-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          [-1.9201, -1.9201, -1.9201,  ..., -1.9201, -1.9201, -1.9201],
          ...,
          [-0.5486, -0.5486, -0.5486,  ..., -0.8229, -1.9201, -1.9201],
          [-0.5486, -0.5486, -0.5486,  ..., -0.8229, -1.9201, -1.9201],
          [-0.2743, -0.2743, -0.2743,  ..., -0.5486, -1.9201, -1.9201]],

         [[-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          [-1.6458, -1.6458, -1.6458,  ..., -1.6458, -1.6458, -1.6458],
          ...,
          [-0.5486, -0.5486, -0.5486,  ..., -0.5486, -1.6458, -1.6458],
          [-0.2743, -0.2743, -0.2743,  ..., -0.5486, -1.6458, -1.6458],
          [ 0.0000,  0.0000,  0.0000,  ..., -0.2743, -1.6458, -1.6458]]]],
       device='cuda:0')
2022-06-25 07:24:52 weight (fake quantized):
2022-06-25 07:24:52 tensor([[[[ 0.0513,  0.1539, -0.0769],
          [ 0.0000, -0.0769,  0.0256],
          [ 0.1539,  0.1539, -0.0513]],

         [[-0.1026, -0.0513,  0.1282],
          [ 0.1795, -0.1795,  0.1282],
          [-0.0256,  0.1795, -0.0513]],

         [[-0.0769,  0.0769,  0.0513],
          [ 0.0256,  0.1026,  0.0256],
          [-0.0769,  0.1795, -0.1282]]],


        [[[-0.0256, -0.0769, -0.1282],
          [ 0.1026,  0.0000,  0.1026],
          [ 0.1795,  0.0513, -0.1026]],

         [[ 0.0513,  0.0769,  0.0256],
          [ 0.0000, -0.1282,  0.0769],
          [-0.0513, -0.1026, -0.0256]],

         [[ 0.0513, -0.1539,  0.0769],
          [-0.1795,  0.1795, -0.0256],
          [ 0.1026, -0.1539, -0.0513]]],


        [[[ 0.0769,  0.0000,  0.0513],
          [ 0.0769, -0.0769, -0.1795],
          [ 0.1539, -0.0256, -0.0256]],

         [[-0.1282,  0.0513, -0.1795],
          [-0.1282,  0.1795, -0.1282],
          [-0.0513,  0.1795, -0.1026]],

         [[ 0.1282, -0.0256,  0.1795],
          [-0.1539,  0.1795, -0.0513],
          [-0.1795,  0.1282,  0.1282]]],


        ...,


        [[[ 0.0769,  0.0256, -0.1539],
          [ 0.0769, -0.0513, -0.1026],
          [-0.1282,  0.1026, -0.1282]],

         [[-0.0769,  0.0000,  0.0513],
          [ 0.0256, -0.0769,  0.0769],
          [ 0.0769, -0.0256, -0.1795]],

         [[ 0.1539,  0.1539, -0.1539],
          [-0.1282, -0.1539, -0.0256],
          [-0.1282, -0.1539, -0.1026]]],


        [[[ 0.1539, -0.1282, -0.1795],
          [ 0.0000, -0.0769,  0.0769],
          [ 0.0513,  0.1795,  0.1026]],

         [[ 0.0256, -0.0769, -0.0769],
          [ 0.1282, -0.0513,  0.1539],
          [ 0.1795, -0.1282,  0.1282]],

         [[ 0.0256, -0.0513, -0.1795],
          [ 0.0000,  0.1539,  0.1026],
          [-0.1539,  0.0769,  0.1282]]],


        [[[ 0.1026,  0.1539,  0.1539],
          [ 0.1539, -0.1539, -0.0769],
          [ 0.0256, -0.1282,  0.0000]],

         [[-0.0769,  0.1026, -0.1282],
          [ 0.1795, -0.0256,  0.1026],
          [ 0.0769,  0.0513,  0.1282]],

         [[-0.0769,  0.1539, -0.1539],
          [-0.0513, -0.0256, -0.0513],
          [ 0.1539, -0.0513,  0.1026]]]], device='cuda:0',
       grad_fn=<FakeQuantOpBackward>)
2022-06-25 07:24:52 input (fake quantized) unfold:
2022-06-25 07:24:52 tensor([[[ 0.0000,  0.0000,  0.0000,  ..., -0.5486,  0.8229,  1.0972],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.8229,  1.0972,  1.0972],
         [ 0.0000,  0.0000,  0.0000,  ...,  1.0972,  1.0972,  0.0000],
         ...,
         [ 0.0000, -1.6458,  0.8229,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458,  0.8229,  1.0972,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.8229,  1.0972, -0.2743,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201,  0.0000],
         ...,
         [ 0.0000,  1.9201,  1.9201,  ...,  0.0000,  0.0000,  0.0000],
         [ 1.9201,  1.9201,  1.3715,  ...,  0.0000,  0.0000,  0.0000],
         [ 1.9201,  1.3715,  0.5486,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000,  ...,  0.2743,  0.2743,  0.2743],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.2743,  0.2743,  0.2743],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.2743,  0.2743,  0.0000],
         ...,
         [ 0.0000, -1.6458,  1.9201,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458,  1.9201,  1.9201,  ...,  0.0000,  0.0000,  0.0000],
         [ 1.9201,  1.9201,  1.9201,  ...,  0.0000,  0.0000,  0.0000]],

        ...,

        [[ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201,  0.0000],
         ...,
         [ 0.0000, -0.8229, -0.8229,  ...,  0.0000,  0.0000,  0.0000],
         [-0.8229, -0.8229, -0.8229,  ...,  0.0000,  0.0000,  0.0000],
         [-0.8229, -0.8229, -0.8229,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -0.8229,  0.2743],
         [ 0.0000,  0.0000,  0.0000,  ..., -0.8229,  0.2743,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.2743,  0.0000,  0.0000],
         ...,
         [ 0.0000, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000,  ..., -0.8229, -0.8229, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -0.8229, -1.9201, -1.9201],
         [ 0.0000,  0.0000,  0.0000,  ..., -1.9201, -1.9201,  0.0000],
         ...,
         [ 0.0000, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000],
         [-1.6458, -1.6458, -1.6458,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:0')
2022-06-25 07:24:52 weight (fake quantized) unfold:
2022-06-25 07:24:52 tensor([[ 0.0513, -0.0256,  0.0769,  ...,  0.0769,  0.1539,  0.1026],
        [ 0.1539, -0.0769,  0.0000,  ...,  0.0256, -0.1282,  0.1539],
        [-0.0769, -0.1282,  0.0513,  ..., -0.1539, -0.1795,  0.1539],
        ...,
        [-0.0769,  0.1026, -0.1795,  ..., -0.1282, -0.1539,  0.1539],
        [ 0.1795, -0.1539,  0.1282,  ..., -0.1539,  0.0769, -0.0513],
        [-0.1282, -0.0513,  0.1282,  ..., -0.1026,  0.1282,  0.1026]],
       device='cuda:0', grad_fn=<TBackward0>)
2022-06-25 07:24:52 input qtensor:
2022-06-25 07:24:52 QTensor(tensor=tensor([[[[ 0, 13, 13,  ...,  4,  4,  3],
          [ 0,  9, 11,  ...,  5,  5,  4],
          [ 0,  7,  7,  ...,  7,  5,  4],
          ...,
          [ 0, 10,  8,  ...,  9, 12, 12],
          [ 0, 10,  9,  ..., 10, 11, 11],
          [ 0, 12, 12,  ...,  6,  9,  9]],

         [[ 0, 13, 13,  ...,  4,  4,  4],
          [ 0,  9, 11,  ...,  5,  5,  5],
          [ 0,  7,  7,  ...,  7,  5,  5],
          ...,
          [ 0, 10,  8,  ..., 10, 12, 12],
          [ 0, 10,  9,  ..., 10, 11, 11],
          [ 0, 12, 12,  ...,  6,  9, 10]],

         [[ 1, 13, 13,  ...,  5,  5,  4],
          [ 1, 10, 11,  ...,  6,  6,  5],
          [ 1,  7,  8,  ...,  8,  5,  5],
          ...,
          [ 1, 10,  8,  ..., 10, 12, 12],
          [ 1, 11, 10,  ..., 10, 11, 11],
          [ 1, 12, 12,  ...,  7,  9, 10]]],


        [[[ 7,  8,  9,  ...,  9,  8,  8],
          [ 9, 10,  9,  ...,  9,  9,  8],
          [12, 11,  7,  ...,  9,  9,  9],
          ...,
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0]],

         [[11, 11, 11,  ..., 12, 12, 12],
          [12, 12, 11,  ..., 12, 12, 12],
          [13, 12,  8,  ..., 12, 12, 12],
          ...,
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0]],

         [[14, 14, 14,  ..., 15, 15, 15],
          [14, 14, 12,  ..., 15, 14, 15],
          [15, 13,  8,  ..., 15, 15, 15],
          ...,
          [ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1]]],


        [[[ 0,  9,  9,  ...,  9,  9,  9],
          [ 0,  9,  9,  ...,  9,  9,  8],
          [ 0,  9,  9,  ...,  9,  9,  9],
          ...,
          [ 0, 12, 12,  ...,  8,  8,  8],
          [ 0, 13, 13,  ...,  8,  8,  8],
          [ 0, 12, 12,  ...,  8,  8,  8]],

         [[ 0, 11, 11,  ..., 11, 11, 11],
          [ 0, 12, 12,  ..., 12, 12, 12],
          [ 0, 12, 12,  ..., 12, 12, 12],
          ...,
          [ 0, 12, 12,  ...,  8,  8,  9],
          [ 0, 13, 13,  ...,  9,  8,  8],
          [ 0, 13, 13,  ...,  8,  8,  8]],

         [[ 1, 13, 13,  ..., 13, 13, 13],
          [ 1, 14, 14,  ..., 14, 14, 14],
          [ 1, 14, 14,  ..., 14, 14, 14],
          ...,
          [ 1, 12, 12,  ...,  9,  9,  9],
          [ 1, 13, 13,  ...,  9,  9,  8],
          [ 1, 13, 13,  ...,  9,  8,  8]]],


        ...,


        [[[ 6,  6,  6,  ...,  0,  0,  0],
          [ 6,  6,  6,  ...,  0,  0,  0],
          [ 6,  6,  6,  ...,  0,  0,  0],
          ...,
          [ 6,  7,  7,  ...,  0,  0,  0],
          [ 5,  6,  6,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0]],

         [[ 4,  4,  4,  ...,  0,  0,  0],
          [ 4,  4,  4,  ...,  0,  0,  0],
          [ 4,  4,  4,  ...,  0,  0,  0],
          ...,
          [ 5,  6,  6,  ...,  0,  0,  0],
          [ 3,  4,  4,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0]],

         [[ 4,  4,  4,  ...,  1,  1,  1],
          [ 4,  4,  4,  ...,  1,  1,  1],
          [ 4,  4,  4,  ...,  1,  1,  1],
          ...,
          [ 3,  4,  4,  ...,  1,  1,  1],
          [ 2,  2,  2,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1]]],


        [[[ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          ...,
          [ 0,  7,  7,  ...,  4,  8,  7],
          [ 0,  6,  6,  ...,  4,  8,  7],
          [ 0,  7,  7,  ...,  5,  6,  7]],

         [[ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          ...,
          [ 0,  7,  7,  ...,  3,  7,  6],
          [ 0,  6,  6,  ...,  4,  7,  6],
          [ 0,  6,  7,  ...,  5,  6,  7]],

         [[ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          ...,
          [ 1,  6,  6,  ...,  4,  7,  6],
          [ 1,  6,  6,  ...,  5,  7,  7],
          [ 1,  7,  7,  ...,  6,  7,  7]]],


        [[[ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          ...,
          [ 5,  4,  4,  ...,  4,  0,  0],
          [ 5,  5,  5,  ...,  4,  0,  0],
          [ 5,  5,  6,  ...,  5,  0,  0]],

         [[ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          [ 0,  0,  0,  ...,  0,  0,  0],
          ...,
          [ 5,  5,  5,  ...,  4,  0,  0],
          [ 5,  5,  5,  ...,  4,  0,  0],
          [ 6,  6,  6,  ...,  5,  0,  0]],

         [[ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          [ 1,  1,  1,  ...,  1,  1,  1],
          ...,
          [ 5,  5,  5,  ...,  5,  1,  1],
          [ 6,  6,  6,  ...,  5,  1,  1],
          [ 7,  7,  7,  ...,  6,  1,  1]]]], device='cuda:0',
       dtype=torch.uint8), scale=tensor(0.2743, device='cuda:0'), zero_point=7)
2022-06-25 07:24:52 weight qtensor:
2022-06-25 07:24:52 QTensor(tensor=tensor([[[[ 9, 13,  4],
          [ 7,  4,  8],
          [13, 13,  5]],

         [[ 3,  5, 12],
          [14,  0, 12],
          [ 6, 14,  5]],

         [[ 4, 10,  9],
          [ 8, 11,  8],
          [ 4, 14,  2]]],


        [[[ 6,  4,  2],
          [11,  7, 11],
          [14,  9,  3]],

         [[ 9, 10,  8],
          [ 7,  2, 10],
          [ 5,  3,  6]],

         [[ 9,  1, 10],
          [ 0, 14,  6],
          [11,  1,  5]]],


        [[[10,  7,  9],
          [10,  4,  0],
          [13,  6,  6]],

         [[ 2,  9,  0],
          [ 2, 14,  2],
          [ 5, 14,  3]],

         [[12,  6, 14],
          [ 1, 14,  5],
          [ 0, 12, 12]]],


        ...,


        [[[10,  8,  1],
          [10,  5,  3],
          [ 2, 11,  2]],

         [[ 4,  7,  9],
          [ 8,  4, 10],
          [10,  6,  0]],

         [[13, 13,  1],
          [ 2,  1,  6],
          [ 2,  1,  3]]],


        [[[13,  2,  0],
          [ 7,  4, 10],
          [ 9, 14, 11]],

         [[ 8,  4,  4],
          [12,  5, 13],
          [14,  2, 12]],

         [[ 8,  5,  0],
          [ 7, 13, 11],
          [ 1, 10, 12]]],


        [[[11, 13, 13],
          [13,  1,  4],
          [ 8,  2,  7]],

         [[ 4, 11,  2],
          [14,  6, 11],
          [10,  9, 12]],

         [[ 4, 13,  1],
          [ 5,  6,  5],
          [13,  5, 11]]]], device='cuda:0', dtype=torch.uint8), scale=tensor(0.0256, device='cuda:0'), zero_point=7)
Traceback (most recent call last):
  File "/content/drive/MyDrive/Any-Precision-DNNs/train.py", line 222, in <module>
    main()
  File "/content/drive/MyDrive/Any-Precision-DNNs/train.py", line 110, in main
    optimizer, sum_writer)
  File "/content/drive/MyDrive/Any-Precision-DNNs/train.py", line 185, in forward
    output = model(input)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/content/drive/MyDrive/Any-Precision-DNNs/models/resnet_quan.py", line 110, in forward
    out = self.conv0(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/content/drive/MyDrive/Any-Precision-DNNs/models/quan_ops.py", line 303, in forward
    return myconv2d_lut(input_qtensor, weight_qtensor, input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
  File "/content/drive/MyDrive/Any-Precision-DNNs/models/quan_ops.py", line 266, in myconv2d_lut
    inp_qtensor_unf = unfold_qtensor(input_qtensor)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/fold.py", line 295, in forward
    self.padding, self.stride)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py", line 4674, in unfold
    if input.dim() == 4:
AttributeError: 'QTensor' object has no attribute 'dim'